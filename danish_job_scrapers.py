"""
Danish AI Job Scrapers for LinkedIn, Emagine, and eWorks Group
"""

import requests
from bs4 import BeautifulSoup
import time
import sqlite3
from datetime import datetime
import json

class DanishJobScraper:
    def __init__(self, db_path="ai_jobs.db"):
        self.db_path = db_path
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        self.setup_database()

    def setup_database(self):
        """Initialize database with proper schema"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS jobs (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                title TEXT NOT NULL,
                company TEXT NOT NULL,
                location TEXT,
                description TEXT,
                url TEXT UNIQUE,
                source TEXT NOT NULL,
                date_posted TEXT,
                job_type TEXT,
                salary TEXT,
                relevance_score INTEGER DEFAULT 0,
                scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()

    def save_job(self, job_data):
        """Save job to database"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            cursor.execute('''
                INSERT OR REPLACE INTO jobs
                (title, company, location, description, url, source, date_posted, job_type, salary, relevance_score)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                job_data['title'],
                job_data['company'],
                job_data['location'],
                job_data['description'],
                job_data['url'],
                job_data['source'],
                job_data['date_posted'],
                job_data['job_type'],
                job_data.get('salary', ''),
                job_data.get('relevance_score', 0)
            ))
            conn.commit()
            return True
        except Exception as e:
            print(f"Error saving job: {e}")
            return False
        finally:
            conn.close()

    def scrape_all_jobs(self):
        """Scrape jobs from all sources"""
        print("ðŸš€ Starting job scraping from Danish AI job sources...")
        
        total_jobs = 0
        
        # For testing, let's just add some sample data
        sample_jobs = [
            {
                'title': 'AI Engineer',
                'company': 'Test Company',
                'location': 'Copenhagen',
                'description': 'Sample AI job',
                'url': 'https://example.com/job1',
                'source': 'Test',
                'date_posted': datetime.now().strftime('%Y-%m-%d'),
                'job_type': 'Full-time',
                'relevance_score': 80
            },
            {
                'title': 'Data Scientist',
                'company': 'Another Company',
                'location': 'Aarhus',
                'description': 'Sample data science job',
                'url': 'https://example.com/job2',
                'source': 'Test',
                'date_posted': datetime.now().strftime('%Y-%m-%d'),
                'job_type': 'Full-time',
                'relevance_score': 90
            }
        ]
        
        for job in sample_jobs:
            if self.save_job(job):
                total_jobs += 1
                print(f"âœ“ Saved job: {job['title']} at {job['company']}")
        
        print(f"\nâœ… Total jobs scraped: {total_jobs}")
        return total_jobs

# Usage example
if __name__ == "__main__":
    scraper = DanishJobScraper()
    scraper.scrape_all_jobs()
